# Disaster Tweets Prediction

## Overview
This project is dedicated to the task of predicting disaster-related tweets, leveraging the invaluable [Kaggle dataset](https://www.kaggle.com/competitions/nlp-getting-started) that has been instrumental in advancing our natural language processing (NLP) endeavors.

## Data Preprocessing
Essential text preprocessing techniques are executed by utilizing the Natural Language Toolkit (NLTK). These techniques encompass the removal of stop words, punctuation, and systematic cleaning of textual data through regular expressions.

## Fine-Tuning BERT
The cornerstone of this prediction model resides in the fine-tuning of the BERT (Bidirectional Encoder Representations from Transformers) architecture, a process executed using PyTorch. This model has attained an accuracy rate of 82.14% on the provided evaluation dataset.

## Future Improvements
Looking ahead, the project's future roadmap involves the exploration and implementation of other BERT-based models such as RoBERTa and DistilBERT. The overarching aim is to further enhance prediction accuracy through meticulous fine-tuning and innovative techniques.
